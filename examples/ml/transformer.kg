:" Transformer implementation in pure Klong
.pyf("numpy";["exp" "max")

:"Define the softmax function"
softmax::{ex::exp(x - max(x)); ex % (+/ex)}

:"Define matrix multiplication (already defined in examples)"
matp::{[b]; b::y; {+/x*b}'x}

:"Define mean function"
mu::{(+/ x) % # x}

:"Define variance function"
var::{((+/ x * x) % # x) - mu(x)^2}

:"Square root"
sqr::{[a];a::x;:[0=x;0;{(x+a%x)%2}:~a]}

:"Define standard deviation function"
sd::{sqr(var(x))}

:"Define layer normalization"
layernorm::{(x - mu(x)) % (sd(x) + 1e-6)}

:"Define the ReLU activation function"
relu::{(x > 0) * x}

:"Define the scaled dot-product attention"
attention::{[Q K V dk scores P]; dk::#K@1; scores::matp(Q;+K) % sqr(dk); P::softmax(scores); matp(P; V)}

:"Define the feed-forward network"
feedforward::{[W1 b1 W2 b2];matp(relu(matp(x; W1) + b1); W2) + b2}

.comment("****")
    W_Q;  :"Weight matrix for Q"
    W_K;  :"Weight matrix for K"
    W_V;  :"Weight matrix for V"
    W_O;  :"Output weight matrix"
    W1;   :"Weight matrix for feed-forward layer 1"
    b1;   :"Bias vector for feed-forward layer 1"
    W2;   :"Weight matrix for feed-forward layer 2"
    b2    :"Bias vector for feed-forward layer 2"
****
transformer::{[WQ WK WV WO W1 b1 W2 b2 Q K V attnoutput x1 ffoutput]; Q::matp(x; WQ); K::matp(x; WK); V::matp(x; WV); attnoutput::attention(Q; K; V); x1::layernorm(x + matp(attnoutput; WO)); ffoutput::feedforward(x1; W1; b1; W2; b2); layernorm(x1 + ff_output)}

:"Compute the Transformer output"
output::transformer(x; [])
