:" ============================================================"
:" Numeric Gradient (∇) vs Autograd (:>)"
:" ============================================================"
:" This example compares the two gradient operators:"
:" - ∇ (nabla): Numeric differentiation using finite differences"
:" - :> : Autograd (uses PyTorch when USE_TORCH=1, else numeric)"
:" Both operators work with any backend."
:" Run with: kgpy numeric_vs_autograd.kg"
:" Or with torch: USE_TORCH=1 kgpy numeric_vs_autograd.kg"
:" ============================================================"

.p("Comparing Gradient Operators")
.p("============================")
.p("")

:" Define test functions"
f::{x^2}
g::{(x^3)+(2*x^2)-(5*x)}
h::{+/x^2}

:" ============================================================"
:" Example 1: Simple scalar function f(x) = x^2"
:" ============================================================"

.p("Example 1: f(x) = x^2, f'(x) = 2x")
.p("----------------------------------")

:" Using ∇ (numeric gradient): syntax is point∇function"
numGrad1::3∇f
.d("  3∇f (numeric) = ")
.p(numGrad1)

:" Using :> (autograd): syntax is function:>point"
autoGrad1::f:>3
.d("  f:>3 (autograd) = ")
.p(autoGrad1)

.p("  Expected: 6.0")
.p("")

:" ============================================================"
:" Example 2: Polynomial g(x) = x^3 + 2x^2 - 5x"
:" g'(x) = 3x^2 + 4x - 5"
:" At x=2: g'(2) = 12 + 8 - 5 = 15"
:" ============================================================"

.p("Example 2: g(x) = x^3 + 2x^2 - 5x, g'(x) = 3x^2 + 4x - 5")
.p("----------------------------------------------------------")

numGrad2::2∇g
.d("  2∇g (numeric) = ")
.p(numGrad2)

autoGrad2::g:>2
.d("  g:>2 (autograd) = ")
.p(autoGrad2)

.p("  Expected: 15.0")
.p("")

:" ============================================================"
:" Example 3: Vector function h(x) = sum(x^2)"
:" Gradient: [2*x1, 2*x2, 2*x3]"
:" ============================================================"

.p("Example 3: h(x) = sum(x^2), gradient = 2x")
.p("------------------------------------------")

numGrad3::[1 2 3]∇h
.d("  [1 2 3]∇h (numeric) = ")
.p(numGrad3)

autoGrad3::h:>[1 2 3]
.d("  h:>[1 2 3] (autograd) = ")
.p(autoGrad3)

.p("  Expected: [2 4 6]")
.p("")

:" ============================================================"
:" Example 4: Precision comparison"
:" ============================================================"

.p("Example 4: Precision comparison with x^10")
.p("------------------------------------------")

:" f(x) = x^10 at x=2"
:" f'(x) = 10*x^9"
:" f'(2) = 10 * 512 = 5120"
poly10::{x^10}

numGradP::2∇poly10
.d("  2∇{x^10} (numeric) = ")
.p(numGradP)

autoGradP::poly10:>2
.d("  {x^10}:>2 (autograd) = ")
.p(autoGradP)

.p("  Expected: 5120.0")
.p("")

:" ============================================================"
:" Summary"
:" ============================================================"

.p("Summary")
.p("-------")
.p("  ∇ (nabla):")
.p("    - Syntax: point∇function")
.p("    - Method: Numeric (finite differences)")
.p("    - Works with any backend")
.p("")
.p("  :> (autograd):")
.p("    - Syntax: function:>point")
.p("    - Method: PyTorch autograd when USE_TORCH=1")
.p("    - Falls back to numeric without torch")
.p("    - Exact gradients with torch, faster for complex functions")
