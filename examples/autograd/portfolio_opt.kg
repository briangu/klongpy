:" ============================================================"
:" Portfolio Optimization with Autograd"
:" ============================================================"
:" Demonstrates using gradient descent to find optimal portfolio"
:" weights that maximize Sharpe ratio subject to constraints."
:" Run with: kgpy portfolio_opt.kg"
:" Or with torch: USE_TORCH=1 kgpy portfolio_opt.kg"
:" ============================================================"

.p("Portfolio Optimization with KlongPy Autograd")
.p("==============================================")
.p("")

:" Import sin/cos from current backend (works with numpy and torch)"
.bkf("sin")
.bkf("cos")

:" Define average function"
avg::{(+/x)%#x}

:" --- Generate Synthetic Return Data ---"

nassets::4
ndays::100

idx::!ndays

:" Use simple pseudo-random based on index"
genReturns::{[a];a::x;((sin(idx*a)+cos(idx*(a+1)))%100)+((a-1.5)%400)}

r0::genReturns(1)
r1::genReturns(2)
r2::genReturns(3)
r3::genReturns(4)

returns::[r0 r1 r2 r3]

.p("Generated synthetic returns for 4 assets over 100 days")
.p("")

:" --- Compute Statistics ---"

mu::252*{avg(x)}'[r0 r1 r2 r3]

vars::{+/((x-avg(x))^2)%#x}'[r0 r1 r2 r3]
vols::{x^0.5}'vars

.d("Expected returns: ")
.p(mu)
.d("Volatilities: ")
.p(vols)
.p("")

:" --- Portfolio Functions ---"

wts::[0.25 0.25 0.25 0.25]

portReturn::{+/x*mu}
portVar::{+/((x^2)*vars)}
portVol::{(portVar(x))^0.5}
sharpe::{portReturn(x)%((portVol(x))+0.0001)}

.d("Initial weights: ")
.p(wts)
.d("Initial Sharpe: ")
.p(sharpe(wts))
.p("")

:" --- Constraint Penalties ---"

sumPenalty::{100*((1-+/x)^2)}
longPenalty::{100*+/((0|0-x)^2)}
loss::{(0-sharpe(x))+sumPenalty(x)+longPenalty(x)}

:" --- Optimization ---"

:" Round to 1 decimal place: multiply by 10, floor, divide by 10"
round1::{(_x*10)%10}

lrp::0.01
epochs::200

.d("Training for ")
.d(epochs)
.d(" epochs with lr=")
.p(lrp)
.p("--------------------------------------------------")

:" Training step - update weights using gradient descent"
:" Returns 0 (scalar) to avoid tensor collection issues with each adverb"
step::{wts::wts-(lrp*(loss:>wts));wts::0|wts;wts::wts%(+/wts);0}

:" Progress printing function"
printProg::{.d("Epoch ");.d(x);.d(": Sharpe=");.d(round1(sharpe(wts)));.d(", weights=");.p(wts)}

:" Run optimization loop - step with progress every 50 epochs"
trainStep::{step();:[0=(x!50);printProg(x);0]}
trainStep'!epochs

.p("")
.p("Optimization complete!")
.d("Final weights: ")
.p(wts)
.d("Final Sharpe: ")
.p(sharpe(wts))
.d("Sum of weights: ")
.p(+/wts)
.p("")

:" --- Analysis ---"
.p("Weight allocation:")
names::["LowVol" "MedVol" "HighVol" "Uncorr"]
showWeight::{pct::round1(100*(wts@x));.d("  ");.d(names@x);.d(": ");.d(pct);.p("%")}
showWeight'!nassets
