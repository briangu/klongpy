:" ============================================================
:" Gradient Descent Fundamentals
:" ============================================================
:"
:" Demonstrates core gradient descent concepts using autograd:
:" 1. Finding minimum of a function
:" 2. Learning rate effects
:" 3. Momentum optimization
:" 4. Fitting a quadratic to data
:"
:" Run with: USE_TORCH=1 kgpy gradient_descent.kg
:" ============================================================

.p("Gradient Descent with KlongPy Autograd")
.p("=======================================")
.p("")

:" ============================================================
:" Example 1: Minimize f(x) = (x-3)^2
:" ============================================================

.p("Example 1: Minimize f(x) = (x-3)^2")
.p("-----------------------------------")
.p("Minimum should be at x = 3")
.p("")

f1::{(x-3)^2}

x::10.0
lr::0.1

.d("Starting x = ";x)
.d("Learning rate = ";lr)
.p("")

step1::{
    grad::f1:>x
    x::x-(lr*grad)
    .if(0=(y!2);{.d("Step ";y;": x=";x;", f(x)=";f1(x);", grad=";grad)};{})
}

step1'1+!10

.p("")
.d("Final x = ";x;" (target: 3.0)")
.p("")

:" ============================================================
:" Example 2: Rosenbrock Function (challenging optimization)
:" ============================================================

.p("Example 2: Rosenbrock Function")
.p("-------------------------------")
.p("f(x,y) = (1-x)^2 + 100*(y-x^2)^2")
.p("Minimum at (1, 1)")
.p("")

:" Rosenbrock as function of parameter vector p = [x, y]
rosenbrock::{[px py];
    px::*x;
    py::x@1;
    ((1-px)^2)+100*((py-(px^2))^2)}

p::[0.0 0.0]
lr::0.001

.d("Starting point: ";p)
.d("Learning rate: ";lr)
.p("")

step2::{
    grad::rosenbrock:>p
    p::p-(lr*grad)
    .if(0=(y!20);{.d("Step ";y;": p=";p;", f(p)=";rosenbrock(p))};{})
}

step2'1+!100

.p("")
.d("Final point: ";p;" (target: [1, 1])")
.p("")

:" ============================================================
:" Example 3: Fitting a Line to Data
:" ============================================================

.p("Example 3: Linear Regression")
.p("-----------------------------")
.p("Fit y = w*x + b to synthetic data")
.p("")

:" Generate synthetic data: y = 2*x + 3 + noise
N::20
idx::!N
X::(4*(idx%N))-2

wTrue::2.0
bTrue::3.0
yTrue::(wTrue*X)+bTrue

.d("True parameters: w=";wTrue;", b=";bTrue)

:" Initialize parameters
w::0.0
b::0.0

:" Loss functions for each parameter
lossW::{((+/((x*X)+b-yTrue)^2))%N}
lossB::{((+/((w*X)+x-yTrue)^2))%N}

lr::0.1

.d("Initial: w=";w;", b=";b)
.p("")

step3::{
    gw::lossW:>w
    gb::lossB:>b
    w::w-(lr*gw)
    b::b-(lr*gb)
    .if(0=(y!5);{.d("Step ";y;": w=";w;", b=";b;", loss=";lossW(w))};{})
}

step3'1+!30

.p("")
.d("Learned: w=";w;", b=";b)
.d("True:    w=";wTrue;", b=";bTrue)
.p("")

:" ============================================================
:" Example 4: Quadratic Curve Fitting
:" ============================================================

.p("Example 4: Quadratic Curve Fitting")
.p("-----------------------------------")
.p("Fit y = a*x^2 + b*x + c to parabola")
.p("")

:" Generate data from y = 0.5*x^2 - 2*x + 1
aTrue::0.5
bTrue2::0-2.0
cTrue::1.0
yQuad::((aTrue*(X^2))+(bTrue2*X))+cTrue

:" Initialize coefficients
a::0.0
b2::0.0
c::0.0

:" Loss for quadratic
quadPred::{((a*(X^2))+(b2*X))+c}
quadLoss::{(+/(quadPred()-yQuad)^2)%N}

:" Separate loss for each coefficient
lossA::{((+/((((x*(X^2))+(b2*X))+c)-yQuad)^2))%N}
lossB2::{((+/(((a*(X^2))+(x*X))+c-yQuad)^2))%N}
lossC::{((+/(((a*(X^2))+(b2*X))+x-yQuad)^2))%N}

lr::0.01

.d("True: a=";aTrue;", b=";bTrue2;", c=";cTrue)
.d("Initial: a=";a;", b=";b2;", c=";c)
.p("")

step4::{
    ga::lossA:>a
    gb::lossB2:>b2
    gc::lossC:>c
    a::a-(lr*ga)
    b2::b2-(lr*gb)
    c::c-(lr*gc)
    .if(0=(y!20);{.d("Step ";y;": a=";a;", b=";b2;", c=";c)};{})
}

step4'1+!100

.p("")
.d("Learned: a=";a;", b=";b2;", c=";c)
.d("True:    a=";aTrue;", b=";bTrue2;", c=";cTrue)
