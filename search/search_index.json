{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"KlongPy","text":"<p>KlongPy is a Python adaptation of the Klong array language, offering high-performance vectorized operations. It prioritizes compatibility with Python, thus allowing seamless integration of Python's expansive ecosystem while retaining Klong's succinctness. With the optional PyTorch backend, KlongPy can operate using both CPU and GPU backends with automatic differentiation support. It utilizes NumPy, an Iverson Ghost descendant from APL, as its core. The framework's foundation lies in Nils M Holm's work, the original developer of Klong, who has authored a Klong Book. KlongPy is especially useful for data scientists, quantitative analysts, researchers, and programming language aficionados.</p>"},{"location":"#quick-install","title":"Quick install","text":"<pre><code>pip3 install \"klongpy[repl]\"   # REPL + NumPy backend\n# or\npip3 install \"klongpy[all]\"    # All extras (torch, web, db, websockets)\n</code></pre> <p>New users may want to read the Quick Start guide and the REPL Reference to get familiar with the interactive environment. Note: the REPL (<code>kgpy</code>) requires the <code>klongpy[repl]</code> extra (or <code>klongpy[all]</code>).</p>"},{"location":"#overview","title":"Overview","text":"<p>KlongPy is both an Array Language runtime and a set of powerful tools for building high performance data analysis and distributed computing applications.  Some of the features include:</p> <ul> <li>Array Programming: Based on Klong, a concise, expressive, and easy-to-understand array programming language. Its simple syntax and rich feature set make it an excellent tool for data scientists and engineers.</li> <li>Speed: Designed for high-speed vectorized computing, enabling you to process large data sets quickly and efficiently on either CPU or GPU.</li> <li>PyTorch Backend &amp; Autograd: Optional PyTorch backend with automatic differentiation for gradient computation, enabling machine learning and optimization workflows.</li> <li>Fast Columnar Database: Includes integration with DuckDB, a super fast in-process columnar store that can operate directly on NumPy arrays with zero-copy.</li> <li>Inter-Process Communication (IPC): Includes built-in support for IPC, enabling easy communication between different processes and systems. Ticker plants and similar pipelines are easy to build.</li> <li>Table and Key-Value Store: Includes a simple file-backed key value store that can be used to store database tables or raw key/value pairs.</li> <li>Python Integration: Seamlessly compatible with Python and modules, allowing you to leverage existing Python libraries and frameworks.</li> <li>Web server: Includes a web server, making it easy to build sites backed by KlongPy capabilities.</li> <li>WebSockets: Connect to WebSocket servers and handle messages in KlongPy.</li> <li>Timers: Includes periodic timer facility to periodically perform tasks.</li> <li>Operator Reference: Quick lookup for language operators.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Consider this simple Klong expression that computes an array's average: <code>(+/a)%#a</code>. Decoded, it means \"sum of 'a' divided by the length of 'a'\", as read from right to left.</p> <p>Below, we define the function 'avg' and apply it to the array of 1 million integers (as defined by !1000000)</p> <p>Let's try this in the KlongPy REPL:</p> <pre><code>$ rlwrap kgpy\n\nWelcome to KlongPy REPL v0.7.0\nauthor: Brian Guarraci\nrepo  : https://github.com/briangu/klongpy\nCtrl-D or ]q to quit\n\n?&gt; avg::{(+/x)%#x}\n:monad\n?&gt; avg(!1000000)\n499999.5\n</code></pre> <p>Now let's time it (first, run it once, then 100 times):</p> <pre><code>?&gt; ]T avg(!1000000)\ntotal: 0.0032962500117719173 per: 0.0032962500117719173\n?&gt; ]T:100 avg(!1000000)\ntotal: 0.10882879211567342 per: 0.0010882879211567343\n</code></pre> <p>We can also import Python custom or standard modules to use directly in Klong language.</p> <pre><code>?&gt; .pyf(\"math\";\"pi\")\n1\n?&gt; pi\n3.141592653589793\n</code></pre> <p>Here we import the fsum function from standard Python math library and redefine avg to use 'fsum':</p> <pre><code>?&gt; .pyf(\"math\";\"fsum\")\n1\n?&gt; favg::{fsum(x)%#x}\n:monad\n?&gt; favg(!1000000)\n499999.5\n</code></pre> <p>Notice that using fsum is slower than using Klong '+/'.  This is because the '+/' operation is vectorized while fsum is not.</p> <pre><code>?&gt; ]T favg(!1000000)\ntotal: 0.050078875152394176 per: 0.050078875152394176\n?&gt; ]T:100 favg(!1000000)\ntotal: 2.93945804098621 per: 0.029394580409862103\n</code></pre> <p>To use KlongPy within Python, here's a basic outline:</p> <pre><code>from klongpy import KlongInterpreter\n\n# instantiate the KlongPy interpreter\nklong = KlongInterpreter()\n\n# define average function in Klong (Note the '+/' (sum over) uses np.add.reduce under the hood)\nklong('avg::{(+/x)%#x}')\n\n# create a billion random uniform values [0,1)\ndata = np.random.rand(10**9)\n\n# reference the 'avg' function in Klong interpreter and call it directly from Python.\nr = klong['avg'](data)\n\nprint(f\"avg={np.round(r,6)}\")\n</code></pre> <p>And let's run a performance comparison between NumPy and PyTorch backends:</p> <pre><code>import time\nimport os\nfrom klongpy import KlongInterpreter\n\n# Use torch backend\nos.environ['USE_TORCH'] = '1'\n\nklong = KlongInterpreter()\nklong('avg::{(+/x)%#x}')\n\n# Create large array\ndata = klong('!1000000')\n\nstart = time.perf_counter_ns()\nr = klong['avg'](data)\nstop = time.perf_counter_ns()\n\nprint(f\"avg={float(r):.6f} in {round((stop - start) / (10**9), 6)} seconds\")\nprint(f\"Backend: {klong._backend.name}, Device: {klong._backend.device}\")\n</code></pre> <p>Run with PyTorch:</p> <pre><code>$ USE_TORCH=1 python3 example.py\navg=499999.5 in 0.001234 seconds\nBackend: torch, Device: mps:0\n</code></pre>"},{"location":"#automatic-differentiation","title":"Automatic Differentiation","text":"<p>With the PyTorch backend, use the <code>:&gt;</code> operator for automatic differentiation:</p> <pre><code>?&gt; f::{x^2}        :\" Define f(x) = x^2\n:monad\n?&gt; f:&gt;3            :\" Compute gradient at x=3\n6.0\n?&gt; g::{+/x^2}      :\" Sum of squares\n:monad\n?&gt; g:&gt;[1 2 3]      :\" Gradient: [2*x1, 2*x2, 2*x3]\n[2.0 4.0 6.0]\n</code></pre> <p>Enable the PyTorch backend with <code>USE_TORCH=1</code> or <code>KLONGPY_BACKEND=torch</code>, or programmatically via <code>KlongInterpreter(backend=\"torch\", device=\"cuda\")</code>.</p> <p>See PyTorch Backend &amp; Autograd for more details and the autograd examples for complete examples including gradient descent and neural networks.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#cpu","title":"CPU","text":"<pre><code>$ pip3 install klongpy\n</code></pre>"},{"location":"#pytorch-backend-recommended-for-autograd-and-gpu","title":"PyTorch Backend (recommended for autograd and GPU)","text":"<pre><code>$ pip3 install \"klongpy[torch]\"\n</code></pre> <p>Then enable with <code>USE_TORCH=1</code>:</p> <pre><code>$ USE_TORCH=1 python your_script.py\n$ USE_TORCH=1 kgpy\n</code></pre>"},{"location":"#all-application-tools-db-web-repl-websockets-etc","title":"All application tools (db, web, REPL, websockets, etc.)","text":"<pre><code>$ pip3 install \"klongpy[all]\"\n</code></pre>"},{"location":"#repl","title":"REPL","text":"<p>KlongPy has a REPL similar to Klong's REPL.</p> <pre><code>$ pip3 install klongpy[repl]\n$ rlwrap kgpy\n\nWelcome to KlongPy REPL\nauthor: Brian Guarraci\nrepo  : https://github.com/briangu/klongpy\nCtrl-C to quit\n\n?&gt; 1+1\n2\n&gt;? \"hello, world!\"\nhello, world!\n?&gt; prime::{&amp;/x!:\\2+!_x^1%2}\n:monad\n?&gt; prime(4)\n0\n?&gt; prime(251)\n1\n?&gt; ]T prime(251)\ntotal: 0.0004914579913020134 per: 0.0004914579913020134\n</code></pre> <p>Read about the prime example here.</p>"},{"location":"#status","title":"Status","text":"<p>KlongPy aims to be a complete implementation of klong.  It currently passes all of the integration tests provided by klong as well as additional suites.</p> <p>The PyTorch backend provides GPU acceleration (CUDA, MPS) and automatic differentiation. Note that the torch backend does not support object dtype arrays or string operations - use the numpy backend for these.</p> <p>Primary ongoing work includes:</p> <ul> <li>Additional tools to make KlongPy applications more capable.</li> <li>Additional syntax error help</li> <li>Expanded torch backend coverage</li> </ul>"},{"location":"#differences-from-klong","title":"Differences from Klong","text":"<p>KlongPy is effectively a superset of the Klong language, but has some key differences:</p> <pre><code>* Infinite precision: The main difference in this implementation of Klong is the lack of infinite precision.  By using NumPy we are restricted to doubles.\n* Python integration: Most notably, the \".py\" command allows direct import of Python modules into the current Klong context.\n* IPC - KlongPy supports IPC between KlongPy processes.\n</code></pre>"},{"location":"#related","title":"Related","text":"<ul> <li>Klupyter - KlongPy in Jupyter Notebooks</li> <li>Advent Of Code '22</li> </ul>"},{"location":"#unused-operators","title":"Unused operators","text":"<p>The following operators are yet to be used:</p> <pre><code>:! :&amp; :, :&lt; :?\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":""},{"location":"#develop","title":"Develop","text":"<pre><code>$ git clone https://github.com/briangu/klongpy.git\n$ cd klongpy\n$ pip install -e \".[dev]\"\n</code></pre>"},{"location":"#running-tests","title":"Running tests","text":"<pre><code>python3 -m pytest tests/\n</code></pre>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>HUGE thanks to Nils M Holm for his work on Klong and providing the foundations for this interesting project.</p>"},{"location":"examples/","title":"KlongPy Examples","text":"<p>Explore the power and simplicity of KlongPy with these engaging examples. Each snippet highlights a unique aspect of Klong, demonstrating its versatility in various programming scenarios.</p> <p>Before we get started, you may be wondering: Why is the syntax so terse?</p> <p>The answer is that it's based on the APL style array language programming and there's a good reason why its compact nature is actually helpful.</p> <p>Array language style lets you describe WHAT you want the computer to do and it lets the computer figure out HOW to do it.  This frees you up from the details while letting the computer figure out how to go as fast as possible.</p> <p>Less code to write and faster execution.</p> <p>Just so the following examples make more sense when you see the REPL outputs, there are a few quick rules about Klong functions.  Functions only take up to 3 parameters and they are ALWAYS called x,y and z.</p> <p>A function with</p> <ul> <li>no parameters is called a nilad</li> <li>one parameter is called a monad (x)</li> <li>two parameters: dyad (x and y)</li> <li>three parameters: a triad (x, y and z)</li> </ul> <p>The reason that Klong functions only take up to 3 parameters AND name them for you is both convenience and compactness.</p>"},{"location":"examples/#1-basic-arithmetic","title":"1. Basic Arithmetic","text":"<p>Let's get started with the basics and build up to some more interesting math.</p> <pre><code>?&gt; 5+3*2 :\" Expressions are evaluated from right to left: 3*2 and then + 5\n11\n</code></pre> <p>KlongPy is more about arrays of things, so let's define sum and count functions over an array:</p> <pre><code>?&gt; sum::{+/x}  :\" sum + over / the array x\n:monad\n?&gt; sum([1 2 3])\n6\n?&gt; count::{#x}\n:monad\n?&gt; count([1 2 3])\n3\n</code></pre> <p>Now that we know the sum and number of elements we can compute the average:</p> <pre><code>?&gt; avg::{sum(x)%count(x)} :\" average is the sum divided by the number of elements\n:monad\n?&gt; avg([1 2 3])\n2\n</code></pre>"},{"location":"examples/#2-math-on-arrays","title":"2. Math on arrays","text":"<p>Let's dig into more interesting operations over array elements.  There's really big performance differences in how you approach the problem and it's important to see the difference.</p> <p>For the simple case of squaring numbers in a list, let's try a couple solutions:</p> <pre><code>?&gt; {x*x}'[1 2 3 4 5] :\" square each element as we iterate over the array\n[1 4 9 16 25]\n</code></pre> <p>The vectorized approach will do an element-wise multiplication in bulk:</p> <pre><code>?&gt; a::[1 2 3 4 5];a*a  :\" a*a multiplies the arrays\n[1 4 9 16 25]\n</code></pre> <p>The vectorized approach is going to be MUCH faster.  Let's crank up the size of the array and time it:</p> <pre><code>$&gt; .l(\"time\")\n:monad\n$&gt; a::!1000;#a\n1\n$&gt; fast::{{a*a}'!1000}\n:nilad\n$&gt; slow::{{{x*x}'a}'!1000}\n:nilad\n$&gt; time(fast)\n0.015867948532104492\n$&gt; time(slow)\n2.8987138271331787\n</code></pre> <p>Vectors win by 182x!  Why?  Because when you perform a bulk vector operation the CPU can perform the math with much less overhead and do many more operations at a time because it has the entire computation presented to it at once.</p> <p>KlongPy aims to give you tools that let you conveniently exploit this vectorization property - and go FAST!</p> <p>Less code to write AND faster to compute.</p>"},{"location":"examples/#3-data-analysis-with-python-integration","title":"3. Data Analysis with Python Integration","text":"<p>KlongPy integrates seamlessly with Python so that the strengths of both can be combined. It's easy to use KlongPy from Python and vice versa.</p> <p>For example, let's say we have some data in Python that we want to operate on in KlongPy.  We can just directly use the interpreter in Python and run functions on data we put into the KlongPy context:</p> <pre><code>from klongpy import KlongInterpreter\nimport numpy as np\n\ndata = np.array([1, 2, 3, 4, 5])\nklong = KlongInterpreter()\n# make the data NumPy array available to KlongPy code by passing it into the interpreter\n# we are creating a symbol in KlongPy called 'data' and assigning the external NumPy array value\nklong['data'] = data\n# define the average function in KlongPY\nklong('avg::{(+/x)%#x}')\n# call the average function with the external data and return the result.\nr = klong('avg(data)')\nprint(r) # expected value: 3\n</code></pre> <p>It doesn't make sense to write code in Klong that already exists in other libraries. We can directly access them via the Python import functions (.py and .pyf).</p> <p>How about we use the NumPy FFT?</p> <pre><code>?&gt; .pyf(\"numpy\";\"fft\");fft::.pya(fft;\"fft\")\n:monad\n?&gt; signal::[0.0 1.0 0.0 -1.0] :\" Example simple signal\n[0.0 1.0 0.0 -1.0]\n?&gt; result::fft(signal)\n[0j -2j 0j 2j]\n</code></pre> <p>Now you can use NumPy or other libraries to provide complex functions while KlongPy lets you quickly prepare and process the vectors.</p> <p>There's a lot more we can do with interop but let's move on for now!</p>"},{"location":"examples/#4-database-functionality","title":"4. Database Functionality","text":"<p>KlongPy leverages a high-performance columnar store called DuckDB that uses zero-copy NumPy array operations behind the scenes. This database allows fast interop between KlongPy and DuckDB (the arrays are not copied) so that applications can manage arrays in KlongPy and then instantly perform SQL on the data for deeper insights.</p> <p>It's easy to create a table and a db to query:</p> <pre><code>?&gt; .py(\"klongpy.db\")\n?&gt; t::.table([[\"name\" [\"Alice\" \"Bob\"]] [\"age\" [25 30]]])\nname age\nAlice 25\nBob 30\n?&gt; db::.db(:{},\"T\",t)\n?&gt; db(\"select * from T where age &gt; 27\")\nname age\nBob 30\n</code></pre>"},{"location":"examples/#5-ipc-remote-function-calls-and-asynchronous-operations","title":"5. IPC, Remote Function Calls and Asynchronous operations","text":"<p>Inter Process Communication (IPC) lets you build distributed and interconnected KlongPy programs and services.</p> <p>KlongPy treats IPC connections to servers as functions. These functions let you call the server and ask for things it has in its memory - they can be other functions or values, etc. For example you can ask for a reference to a remote function and you will get a local function that when you call it runs on the server with your arguments. This general \"remote proxy\" approach allows you to write your client code in the same way as if all the code were running locally.</p> <p>To see this in action, let's setup a simple scenario where the server has an \"avg\" function and the client wants to call it.</p> <p>Start a server in one terminal:</p> <pre><code>?&gt; avg::{(+/x)%#x}\n:monad\n?&gt; .srv(8888)\n1\n</code></pre> <p>Start the client and make the connection to the server as 'f'.  In order to pass parameters to a remote function we form an array of the function symbol followed by the parameters (e.g. :avg,,!100)</p> <pre><code>?&gt; f::.cli(8888) :\" connect to the server\nremote[localhost:8888]:fn\n?&gt; f(:avg,,!100) :\" call the remote function avg directly with the parameter !100\n49.5\n</code></pre> <p>Let's get fancy and make a local proxy to the remote function:</p> <pre><code>?&gt; myavg::f(:avg) :\" reference the remote function by it's symbol :avg and assign to a local variable called myavg\nremote[localhost:8888]:fn:avg:monad\n?&gt; myavg(!100) :\" this runs on the server with !100 array passed to it as a parameter\n49.5\n</code></pre> <p>Since remote functions may take a while we can wrap them with an async wrapper and have it call our callback when completed:</p> <pre><code>?&gt; afn::.async(myavg;{.d(\"Avg calculated: \");.p(x)})\nasync::monad\n?&gt; afn(!100)\nAvg calculated: 49.5\n1\n</code></pre>"},{"location":"examples/#6-web-server-implementation","title":"6. Web Server Implementation","text":"<p>In addition to IPC we can also expose data via a standard web server.  This capability lets you have other ways of serving content that can be either exposing interesting details about some computation or just a simple web server for other reasons.</p> <p>Let's create a file called web.kg with the following code that adds one index handler:</p> <pre><code>.py(\"klongpy.web\")\ndata::!10\nindex::{x; \"Hello, Klong World! \",data}\n.web(8888;:{},\"/\",index;:{})\n.p(\"ready at http://localhost:8888\")\n</code></pre> <p>We can run this web server as follows:</p> <pre><code>$ kgpy web.kg\nready at http://localhost:8888\n</code></pre> <p>In another terminal:</p> <pre><code>$ curl http://localhost:8888\n['Hello, Klong World! ' 0 1 2 3 4 5 6 7 8 9]\n</code></pre>"},{"location":"examples/#7-automatic-differentiation-autograd","title":"7. Automatic Differentiation (Autograd)","text":"<p>KlongPy supports automatic differentiation, enabling gradient-based optimization and machine learning workflows.</p>"},{"location":"examples/#numeric-gradient-with-always-numeric-any-backend","title":"Numeric Gradient with <code>\u2207</code> (always numeric, any backend)","text":"<p>The <code>\u2207</code> operator always computes gradients using numeric differentiation:</p> <pre><code>?&gt; f::{x^2}        :\" Define f(x) = x^2\n:monad\n?&gt; 3\u2207f             :\" Compute f'(3) \u2248 6.0\n6.0\n</code></pre>"},{"location":"examples/#pytorch-autograd-with-recommended-with-torch-backend","title":"PyTorch Autograd with <code>:&gt;</code> (recommended with torch backend)","text":"<p>Enable the torch backend for exact gradients:</p> <pre><code>$ USE_TORCH=1 kgpy\n</code></pre> <p>Use the <code>:&gt;</code> operator for PyTorch autograd. The syntax is <code>function:&gt;point</code>:</p> <pre><code>?&gt; f::{x^2}        :\" Define f(x) = x^2\n:monad\n?&gt; f:&gt;3            :\" Compute f'(3) = 2*3 = 6\n6.0\n</code></pre> <p>For vector-valued inputs, the gradient is computed element-wise:</p> <pre><code>?&gt; h::{+/x^2}      :\" h(x) = sum of squares\n:monad\n?&gt; h:&gt;[1.0 2.0 3.0]   :\" Gradient: [2*1, 2*2, 2*3]\n[2.0 4.0 6.0]\n</code></pre> <p>Simple gradient descent to minimize x^2:</p> <pre><code>?&gt; f::{x^2}\n:monad\n?&gt; x::5.0; lr::0.1\n0.1\n?&gt; x::x-(lr*f:&gt;x); x  :\" One gradient step\n4.0\n?&gt; x::x-(lr*f:&gt;x); x  :\" Another step\n3.2\n</code></pre> <p>For complete examples including linear regression and neural networks, see the autograd examples.</p>"},{"location":"examples/#conclusion","title":"Conclusion","text":"<p>These examples are designed to illustrate the ease of use and diverse applications of Klong, making it a versatile choice for various programming needs.</p> <p>Check out the references for details and deep dives on specific functionality.</p>"},{"location":"fast_columnar_database/","title":"Fast Columnar Database","text":"<p>KlongPy provides a module <code>klongpy.db</code> that includes DuckDB integration. DuckDB can operate directly on NumPy arrays, which allows for zero-copy SQL execution over pre-existing NumPy data.</p> <p>Install the database extras first:</p> <pre><code>pip install \"klongpy[db]\"\n</code></pre>"},{"location":"fast_columnar_database/#tables","title":"Tables","text":"<pre><code>?&gt; .py(\"klongpy.db\")\n?&gt; t::.table([[\"a\" [1 2 3]] [\"b\" [2 3 4]]])\na b\n1 2\n2 3\n3 4\n?&gt; t,\"c\",,[3 4 5]\na b c\n1 2 3\n2 3 4\n3 4 5\n</code></pre> <p>You can also create a table directly from a Pandas DataFrame.</p> <p>Indexes (one or more columns) can be created on a table. The current indexes can be seen in the table description prefix.</p> <pre><code>?&gt; .index(t; [\"a\"])\n['a']\n</code></pre> <p>When a column is indexed, it appears with an asterisk in the pretty-print format:</p> <pre><code>?&gt; t\na* b\n 1 2\n 2 3\n 3 5\n</code></pre> <p>Inserting a row with a pre-existing value at an index results in an update:</p> <pre><code>?&gt; .insert(t, [3 5 6])\na* b c\n 1 2 3\n 2 3 4\n 3 5 6\n</code></pre> <p>Indexes may be reset via .rindex().  True is returned if the index was reset.</p> <pre><code>?&gt; .rindex(t)\n1\n</code></pre>"},{"location":"fast_columnar_database/#database","title":"Database","text":"<p>Databases are created from a map of table names to table instances.  A database instance is a function which accepts SQL and runs it over the underlying tables.  SQL results are NumPy arrays and can be directly used in normal KlongPy operations.</p> <pre><code>?&gt; T::.table(,\"a\",,[1 2 3])\na\n1\n2\n3\n?&gt; db::.db(:{},\"T\",,T)\n:db\n?&gt; db(\"select * from T\")\n[1 2 3]\n</code></pre> <p>Since KlongPy uses DuckDB under the hood, you can perform sophisticated SQL over the underlying NumPy arrays.</p> <p>For example, it's easy to use JOIN with this setup:</p> <pre><code>d::[]\nd::d,,\"a\",,[1 2 3]\nd::d,,\"b\",,[2 3 4]\nT::.table(d)\n\ne::,\"c\",,[3 4 5]\nG::.table(e)\n\nq:::{}\nq,\"T\",,T\nq,\"G\",,G\ndb::.db(q)\n</code></pre> <p>We can now issue a JOIN SQL:</p> <pre><code>?&gt; db(\"select * from T join G on G.c = T.b\")\n[[2 3 3]\n [3 4 4]]\n</code></pre>"},{"location":"fast_columnar_database/#pandas-dataframe-integration","title":"Pandas DataFrame integration","text":"<p>Tables are backed by Pandas DataFrames, so it's easy to integrate Pandas directly into KlongPy via DuckDB.</p> <pre><code>from klongpy import KlongInterpreter\nimport pandas as pd\n\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n        'Age': [25, 30, 35, 40]}\ndf = pd.DataFrame(data)\n\nklong = KlongInterpreter()\nklong['df'] = df\nr = klong(\"\"\"\n.py(\"klongpy.db\")\nt::.table(df)\ndb::.db(:{},\"people\",t)\ndb(\"select Age from people\")\n\"\"\")\n</code></pre>"},{"location":"ipc_capabilities/","title":"Inter-Process Communication (IPC) Capabilities","text":"<p>KlongPy has powerful Inter-Process Communication (IPC) features that enable it to connect and interact with remote KlongPy instances. This includes executing commands, retrieving or storing data, and even defining functions remotely. These new capabilities are available via two new functions: .cli() and .clid().</p>"},{"location":"ipc_capabilities/#the-cli-function","title":"The .cli() Function","text":"<p>The .cli() function creates an IPC client. You can pass it either an integer (interpreted as a port on \"localhost:\"), a string (interpreted as a host address \":\"), or a remote dictionary (which shares the network connection and returns a remote function). <p>Use .cli() to evaluate commands on a remote KlongPy server, define functions, perform calculations, or retrieve values. You can also pass it a symbol to retrieve a value or a function from the remote server.</p> <p>Start the IPC server:</p> <pre><code>$ kgpy -s 8888\nWelcome to KlongPy REPL v0.7.0\nauthor: Brian Guarraci\nrepo  : https://github.com/briangu/klongpy\nCtrl-D or ]q to quit\n\nRunning IPC server at 8888\n\n?&gt;\n</code></pre> <p>In a different terminal:</p> <pre><code>$ kgpy\n\n?&gt; f::.cli(8888)\nremote[localhost:8888]:fn\n?&gt; f(\"avg::{(+/x)%#x}\")\n:monad\n?&gt; f(\"avg(!100)\")\n49.5\n?&gt; :\" Call a remote function and pass a local value (!100) \"\n?&gt; data::!100\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n 96 97 98 99]\n?&gt; f(:avg,,data)\n49.5\n</code></pre> <p>Using remote function proxies, you can reference a remotely defined function and call it as if it were local:</p> <pre><code>?&gt; q::f(:avg)\nremote[localhost:8888]:avg:monad\n?&gt; q(!100)\n49.5 \n</code></pre>"},{"location":"ipc_capabilities/#the-clid-function","title":"The .clid() Function","text":"<p>As seen in Python interop examples, the KlongPy context is effectively a dictionary.  The .clid() function creates an IPC client that treats the remote KlongPy context as a dictionary, allowing you to set/get values on the remote instance.  Combined with the remote function capabilities, the remote dictionary makes it easy to interact with remote KlongPy instances.</p> <p>Here are some examples:</p> <pre><code>?&gt; :\" Open a remote dictionary using the same connection as f \"\n?&gt; d::.clid(f)\nremote[localhost:8888]:dict\n?&gt; :\" Add key/value pair :foo -&gt; 2 to remote context \"\n?&gt; d,[:foo 2]\n?&gt; :\" Get the value for :foo key from the remote context \"\n?&gt; d?:foo\n2\n?&gt; d,[:bar \"hello\"]\n?&gt; d?:bar\nhello\n?&gt; :\" Assign a remote function to :fn \"\n?&gt; d,:fn,{x+1}\n?&gt; t::d?:fn\nremote[localhost:8888]:fn:monad\n?&gt; t(10)\n11\n</code></pre> <p>These powerful capabilities allow for more effective use of distributed computing resources. Please be aware of potential security issues, as you are allowing a remote server to execute potentially arbitrary commands from your client. Always secure your connections and validate your commands to avoid potential attacks.</p>"},{"location":"ipc_capabilities/#remote-function-proxies-and-enumeration","title":"Remote Function Proxies and Enumeration","text":"<p>Another powerful feature of KlongPy's IPC capabilities is the use of remote function proxies. These function proxies behave as if they were local functions, but are actually executed on a remote server. You can easily create these function proxies using .cli() or .clid(), and then use them as you would any other function.</p> <p>One of the most powerful aspects of these remote function proxies is that they can be stored in an array and then enumerated. When you do this, KlongPy will execute each function in turn with the specified parameters.</p> <p>For example, suppose you have created three remote function proxies:</p> <pre><code>?&gt; d::.clid(8888)\n?&gt; d,:avg,{(+/x)%#x}\n?&gt; d,:sum,{(+/x)}\n?&gt; d,:max,{(x@&gt;x)@0}\n?&gt; a = d?:avg\n?&gt; b = d?:sum\n?&gt; c = d?:max\n</code></pre> <p>You can then call each of these functions with the same parameter by using enumeration:</p> <pre><code>?&gt; {x@,!100}'[a b c]\n[  49.5 4950.    99. ]\n</code></pre> <p>In this example, KlongPy will execute each function with the range 0-99 as a parameter, and then store the results in the results array. The :avg function will calculate the average of the numbers, the :sum function will add them up, and the :max function will return the largest number in the range.</p> <p>This makes it easy to perform multiple operations on the same data set, or to compare the results of different functions. It's another way that KlongPy's IPC capabilities can enhance your data analysis and distributed computing tasks.</p>"},{"location":"ipc_capabilities/#closing-remote-function-proxies","title":"Closing Remote Function Proxies","text":"<p>Closing remote connections is done with the .clic() command.  Once it is closed, all proxies that shared that connection are now disconnected as well.</p> <pre><code>?&gt; f::.cli(8888)\n?&gt; .clic(f)\n1\n</code></pre>"},{"location":"ipc_capabilities/#async-function-calls","title":"Async function calls","text":"<p>KlongPy supports async function calls. While it works for local functions, it's primarily for remote functions.</p> <p>To indicate a function call should be async, the .async() function wraps the function and the supplied callback is called when complete.</p> <p>Calling an async function results in 1, indicating it was executed.</p> <pre><code>?&gt; fn::f(:avg)\nremote[localhost:8888]:avg:monad\n?&gt; cb::{.d(\"remote done: \");.p(x)}\n:monad\n?&gt; afn::.async(fn;cb)\nasync:monad\n?&gt; afn(!100)\n1\n?&gt; remote done: 49.5\n</code></pre> <p>Note, the result of .async() is a function, so it's possible to reuse these.</p>"},{"location":"ipc_capabilities/#synchronization","title":"Synchronization","text":"<p>While the IPC server I/O is async, the KlongPy interpreter is single-threaded.  All remote operations are synchronous to make it easy to use remote operations as part of a normal workflow.  Of course, when calling over to another KlongPy instance, you have no idea what state that instance is in, but within the calling instance operations will be sequential.</p>"},{"location":"ipc_capabilities/#server-callbacks","title":"Server Callbacks","text":"<p>The KlongPy IPC server has 3 connection-related callbacks that can be assigned to pre-defined symbols:</p>"},{"location":"ipc_capabilities/#client-connection-open-srvo","title":"Client connection open: <code>.srv.o</code>","text":"<p>Called when a new client connection is established.  The argument passed is the remote connection handle (fn) to the connecting client.  Note, handler functions should not call back to the client when called as it will produce a deadlock - the client is in the process of connecting to the server and not servicing requests.</p> <pre><code>.srv.o::{.d(\"client has connected: \");.p(x)}\n</code></pre>"},{"location":"ipc_capabilities/#client-connection-close-srvc","title":"Client connection close: <code>.srv.c</code>","text":"<p>Called when a client disconnects or drops the connection due to an error.  The passed argument is the client handle similar to <code>.srv.o</code>.</p> <pre><code>.srv.c::{.d(\"client has disconnected: \");.p(x)}\n</code></pre>"},{"location":"ipc_capabilities/#client-connection-error-srve","title":"Client connection error: <code>.srv.e</code>","text":"<p>Called when there is a client error condition.  Arguments are the client handle and the exception that caused the error.</p> <pre><code>.srv.e::{.d(\"client has had an error: \");.d(x);.d(\" \");.p(y)}\n</code></pre>"},{"location":"ipc_capabilities/#building-a-pub-sub-example","title":"Building a pub-sub example","text":"<p>Using the server callbacks, it's easy to set up a pub-sub example where a client connects and then subscribes to a server. Periodically the server will call the update method on the client with new data.</p> <p>Server:</p> <pre><code>:\"broadcast fake stock data to all subscribed clients\"\n\n:\" Map of clients handles to their subscribed tickers \"\nclients:::{}\n\n:\" Called by clients to subscribe to ticker updates \"\nsubscribe::{.d(\"subscribing client: \");.p(x);clients,.cli.h,,(clients?.cli.h),,x;.p(clients)}\n\n:\" Periodically called to broadcast updates to all subscribed clients \"\nsend::{.d(\"sending to client\");.p(x);x(:update,,{x,.rn()*50}'y)}\nbroadcast::{.p(\"sending messages to clients\");{send(x@0;x@1)}'clients}\ncb::{:[(#clients)&gt;0;broadcast();.p(\"no clients to broadcast to\")];1}\nth::.timer(\"ticker\";1;cb)\n\n:\" Setup the IPC server and callbacks \"\n.srv(8888)\n.srv.o::{.d(\"client connected: \");.p(x);clients,x,,[]}\n.srv.c::{.d(\"client disconnected: \");.p(x);x_clients;.d(\"clients left: \");.p(#clients)}\n.srv.e::{.d(\"error: \");.p(x);.p(y)}\n</code></pre> <p>Client</p> <pre><code>:\"Connect to the broadcast server\"\n\n.p(\"connecting to server on port 8888\")\n\ncli::.cli(8888)\n.p(cli)\n\n:\" Called by server when there is a subscription update.\"\nupdate::{.d(\"subscription update: \");.p(x)}\n\ncli(:subscribe,,[\"MSFT\" \"GOOG\" \"AAPL\"])\n</code></pre> <p>Running these is easy:</p> <pre><code>$ kgpy examples/ipc/srv_pubsub.kg\nno clients to broadcast to\nno clients to broadcast to\n...\n</code></pre> <p>One we run the client, the server will begin to send updates to the client:</p> <pre><code>$ kgpy examples/ipc/cli_pubsub.kg\nconnecting to server on port 8888\nremote[localhost:8888]:fn\nsubscription update: [MSFT 16.310530573710896 GOOG 27.199690444331594 AAPL 35.81725374157503]\nsubscription update: [MSFT 43.28567690091258 GOOG 32.06719233158067 AAPL 47.306031721530864]\n</code></pre>"},{"location":"operators/","title":"Operator Reference","text":""},{"location":"operators/#monads-single-argument","title":"Monads (Single Argument)","text":"Operator Name Description <code>@a</code> Atom Check if value is an atom <code>:#a</code> Char Convert to character <code>!a</code> Enumerate Generate sequence 0..a-1 <code>&amp;a</code> Expand Expand boolean mask <code>*a</code> First First element <code>_a</code> Floor Floor/truncate <code>$a</code> Format Convert to string <code>&gt;a</code> Grade-Down Indices that would sort descending <code>&lt;a</code> Grade-Up Indices that would sort ascending <code>=a</code> Group Group equal elements <code>,a</code> List Wrap in list <code>-a</code> Negate Negate value <code>~a</code> Not Logical not <code>%a</code> Reciprocal 1/a <code>?a</code> Range Random or range <code>|a</code> Reverse Reverse order <code>^a</code> Shape Array dimensions <code>#a</code> Size Length/count <code>+a</code> Transpose Transpose matrix <code>:_a</code> Undefined Check if undefined <code>\u2207a</code> Grad Numeric gradient function of a"},{"location":"operators/#dyads-two-arguments","title":"Dyads (Two Arguments)","text":"Operator Name Description <code>a:=b</code> Amend Amend array at index <code>a:-b</code> Amend-in-Depth Deep amend <code>a:_b</code> Cut Cut array at indices <code>a::b</code> Define Define variable <code>a%b</code> Divide Division <code>a_b</code> Drop Drop elements <code>a=b</code> Equal Equality test <code>a?b</code> Find Find element <code>a:$b</code> Form Format with precision <code>a$b</code> Format2 Format with width <code>a@b</code> Index/Apply Index or apply function <code>a:@b</code> Index-in-Depth Deep indexing <code>a:%b</code> Integer-Divide Integer division <code>a,b</code> Join Concatenate <code>a&lt;b</code> Less Less than <code>a~b</code> Match Deep equality <code>a\\|b</code> Max/Or Maximum or logical or <code>a&amp;b</code> Min/And Minimum or logical and <code>a-b</code> Minus Subtraction <code>a&gt;b</code> More Greater than <code>a+b</code> Plus Addition <code>a^b</code> Power Exponentiation <code>a:^b</code> Reshape Reshape array <code>a!b</code> Remainder Modulo <code>a:+b</code> Rotate Rotate elements <code>a:#b</code> Split Split at indices <code>a#b</code> Take Take elements <code>a*b</code> Times Multiplication <code>a\u2207b</code> Grad Numeric gradient of b at a <code>a:&gt;b</code> Autograd Gradient of a at b (PyTorch autograd)"},{"location":"operators/#gradient-operators","title":"Gradient Operators","text":"<p>KlongPy provides two gradient operators:</p>"},{"location":"operators/#numeric-gradient-nabla","title":"Numeric Gradient: <code>\u2207</code> (nabla)","text":"<p>The <code>\u2207</code> operator always computes gradients using numeric differentiation (finite differences), regardless of which backend is active.</p> <p>Dyad syntax: <code>point\u2207function</code></p> <pre><code>f::{x^2}\n3\u2207f               :\" Returns ~6.0 (derivative of x^2 at x=3)\n\ng::{+/x^2}\n[1 2 3]\u2207g         :\" Returns [2 4 6] (gradient of sum of squares)\n</code></pre> <p>Monad syntax: <code>\u2207function</code> returns a gradient function</p> <pre><code>f::{x^2}\ngrad_f::\u2207f        :\" Create gradient function\ngrad_f(3)         :\" Compute gradient at x=3\n</code></pre>"},{"location":"operators/#pytorch-autograd","title":"PyTorch Autograd: <code>:&gt;</code>","text":"<p>The <code>:&gt;</code> operator uses PyTorch's automatic differentiation when the torch backend is enabled (<code>USE_TORCH=1</code>). Falls back to numeric differentiation otherwise.</p> <p>Syntax: <code>function:&gt;point</code></p> <pre><code>f::{x^2}\nf:&gt;3              :\" Returns 6.0 (derivative of x^2 at x=3)\n\ng::{+/x^2}\ng:&gt;[1 2 3]        :\" Returns [2 4 6] (gradient of sum of squares)\n</code></pre>"},{"location":"operators/#multi-parameter-gradients","title":"Multi-Parameter Gradients","text":"<p>The <code>:&gt;</code> operator can compute gradients for multiple parameters simultaneously when given a list of symbols:</p> <pre><code>w::2.0;b::3.0\nloss::{(w^2)+(b^2)}\n\n:\" Single parameter (returns gradient)\nloss:&gt;w               :\" Returns 4.0\n\n:\" Multiple parameters (returns list of gradients)\nloss:&gt;[w b]           :\" Returns [4.0 6.0]\n</code></pre>"},{"location":"operators/#jacobian-operator","title":"Jacobian Operator: <code>\u2202</code>","text":"<p>The <code>\u2202</code> (partial derivative) operator computes the Jacobian matrix:</p> <p>Syntax: <code>point\u2202function</code></p> <pre><code>f::{x^2}              :\" Element-wise square\n[1 2]\u2202f               :\" [[2 0] [0 4]] - diagonal Jacobian\n\ng::{+/x^2}            :\" Sum of squares (scalar output)\n[1 2 3]\u2202g             :\" [2 4 6] - gradient vector\n</code></pre> <p>Also available as <code>.jacobian(function;point)</code>.</p>"},{"location":"operators/#multi-parameter-jacobians","title":"Multi-Parameter Jacobians","text":"<p>Like gradients, Jacobians can be computed for multiple parameters using a list of symbols:</p> <pre><code>w::[1.0 2.0]\nb::[3.0 4.0]\nf::{w^2}              :\" Returns [w0^2, w1^2]\n\n:\" Compute Jacobians for both w and b\n[w b]\u2202f               :\" Returns [J_w, J_b]\n</code></pre>"},{"location":"operators/#comparison","title":"Comparison","text":"Feature <code>\u2207</code> (nabla) <code>:&gt;</code> (autograd) <code>\u2202</code> (jacobian) Method Numeric PyTorch autograd PyTorch/numeric Precision Approximate Exact Exact Output Scalar functions Scalar functions Vector functions Syntax <code>point\u2207function</code> <code>function:&gt;point</code> <code>point\u2202function</code> Multi-param No <code>f:&gt;[w b]</code> <code>[w b]\u2202f</code>"},{"location":"operators/#autograd-system-functions","title":"Autograd System Functions","text":"<p>Additional system functions for autograd (require PyTorch backend):</p> Function Description <code>.gradcheck(fn;input)</code> Verify gradients against numeric computation <code>.compile(fn;input)</code> Compile function for optimized execution <code>.compilex(fn;input;opts)</code> Compile with extended options (mode, backend) <code>.cmodes()</code> Query available compilation modes and backends <code>.export(fn;input;path)</code> Export computation graph to file <p>Gradient Verification: <pre><code>f::{x^2}\n.gradcheck(f;3.0)     :\" Returns 1 if gradients are correct\n</code></pre></p> <p>Function Compilation: <pre><code>f::{(x^3)+(2*x^2)+x}\ncf::.compile(f;2.0)   :\" Returns compiled (faster) function\ncf(5.0)               :\" Execute compiled function\n</code></pre></p> <p>Extended Compilation: <pre><code>:\" Fast compile for development\ncf::.compilex(f;2.0;:{[\"mode\" \"reduce-overhead\"]})\n\n:\" Maximum optimization for production\ncf::.compilex(f;2.0;:{[\"mode\" \"max-autotune\"]})\n\n:\" Debug mode (no C++ compiler needed)\ncf::.compilex(f;2.0;:{[\"backend\" \"eager\"]})\n\n:\" Query available options\ninfo::.cmodes()\n</code></pre></p> <p>Graph Export: <pre><code>info::.export(f;2.0;\"model.pt2\")\n.p(info@\"graph\")      :\" Print computation graph\n</code></pre></p>"},{"location":"operators/#adverbs","title":"Adverbs","text":"Adverb Name Description <code>f'a</code> Each Apply f to each element <code>f/a</code> Over Reduce with f <code>f\\a</code> Scan Scan with f <code>n f'a</code> Each-n Apply f to groups of n <code>a f'b</code> Each-pair Apply f to pairs <code>f:*a</code> Iterate Iterate f until stable <code>n f:*a</code> Iterate-n Iterate f n times <code>f:~a</code> Converge Converge with condition"},{"location":"operators/#unused-operators","title":"Unused Operators","text":"<p>The following operator combinations are reserved for future use:</p> <pre><code>:! :&amp; :, :&lt; :?\n</code></pre>"},{"location":"performance/","title":"Performance","text":"<p>The Klong language is simple, so the overhead is low. The bulk of the compute time will likely be spent in the array backend doing actual work.</p> <p>Key insight: GPU acceleration shines for compute-bound operations (matrix multiply), not memory-bound operations (element-wise ops).</p> <p>Use <code>USE_TORCH=1</code> or <code>KLONGPY_BACKEND=torch</code> to enable the PyTorch backend.</p>"},{"location":"performance/#benchmark","title":"Benchmark","text":"<p>The benchmark (<code>tests/perf_vector.py</code>) tests two workload types:</p> <ol> <li>Vector ops (element-wise, memory-bound): <code>2*1+!10000000</code></li> <li>Matrix multiply (compute-bound): 4000\u00d74000 matmul</li> </ol>"},{"location":"performance/#results","title":"Results","text":""},{"location":"performance/#cuda-gpu","title":"CUDA GPU","text":"<pre><code>$ python3 tests/perf_vector.py\n============================================================\nVECTOR OPS (element-wise, memory-bound)\n  Size: 10,000,000 elements, Iterations: 100\n============================================================\nNumPy (baseline)                    0.021854s\nKlongPy (numpy)                     0.001413s  (15.46x vs NumPy)\nKlongPy (torch, cpu)                0.000029s  (761.22x vs NumPy)\nKlongPy (torch, cuda)               0.000028s  (784.04x vs NumPy)\n\n============================================================\nMATRIX MULTIPLY (compute-bound, GPU advantage)\n  Size: 4000x4000, Iterations: 5\n============================================================\nNumPy (baseline)                    0.078615s\nKlongPy (numpy)                     0.075400s  (1.04x vs NumPy)\nKlongPy (torch, cpu)                0.077350s  (1.02x vs NumPy)\nKlongPy (torch, cuda)               0.002339s  (33.62x vs NumPy)\n</code></pre>"},{"location":"performance/#apple-silicon-mps","title":"Apple Silicon (MPS)","text":"<pre><code>$ python3 tests/perf_vector.py\n============================================================\nVECTOR OPS (element-wise, memory-bound)\n  Size: 10,000,000 elements, Iterations: 100\n============================================================\nNumPy (baseline)                    0.007547s\nKlongPy (numpy)                     0.007378s  (1.02x vs NumPy)\nKlongPy (torch, cpu)                0.007360s  (1.03x vs NumPy)\nKlongPy (torch, mps)                0.007320s  (1.03x vs NumPy)\n\n============================================================\nMATRIX MULTIPLY (compute-bound, GPU advantage)\n  Size: 4000x4000, Iterations: 5\n============================================================\nNumPy (baseline)                    0.034870s\nKlongPy (numpy)                     0.036095s  (0.97x vs NumPy)\nKlongPy (torch, cpu)                0.035907s  (0.97x vs NumPy)\nKlongPy (torch, mps)                0.011203s  (3.11x vs NumPy)\n</code></pre> <p>Observations: - CUDA: Massive speedups for both vector ops (784x) and matrix multiply (34x) - MPS: ~3x speedup for matrix multiply; vector ops similar across backends - Results vary by hardware and workload characteristics</p> <p>See torch_backend.md for more details on the PyTorch backend and performance characteristics.</p>"},{"location":"python_integration/","title":"Python Integration","text":"<p>Seamlessly blending Klong and Python is the cornerstone of KlongPy, enabling you to utilize each language where it shines brightest. For instance, you can integrate KlongPy into your ML/Pandas workflows, or deploy it as a powerhouse driving your website backend.</p> <p>The charm of KlongPy lies in its dictionary-like interpreter that hosts the current KlongPy state, making it incredibly simple to extend KlongPy with custom functions and shuttle data in and out of the interpreter.</p> <p>Imagine your data processed elsewhere, just set it into KlongPy and watch as the Klong language works its magic, accessing and manipulating your data with effortless ease. Even more, Python lambdas or functions can directly be exposed as Klong functions, adding an array of powerful tools to your Klong arsenal.</p> <p>KlongPy indeed is a force multiplier, amplifying the power of your data operations.</p>"},{"location":"python_integration/#function-example","title":"Function example","text":"<p>Call a Python function from Klong:</p> <pre><code>from klongpy import KlongInterpreter\nklong = KlongInterpreter()\nklong['f'] = lambda x, y, z: x*1000 + y - z\nr = klong('f(3; 10; 20)')\nassert r == 2990\n</code></pre> <p>and vice versa, you can call a Klong function from Python:</p> <pre><code>from klongpy import KlongInterpreter\nklong = KlongInterpreter()\nklong(\"f::{(x*1000) + y - z}\")\nr = klong['f'](3, 10, 20)\nassert r == 2990\n</code></pre>"},{"location":"python_integration/#data-example","title":"Data example","text":"<p>Since the Klong interpreter context is dictionary-like, you can store values there for access in Klong:</p> <pre><code>import numpy as np\nfrom klongpy import KlongInterpreter\n\nklong = KlongInterpreter()\ndata = np.arange(10*9)\nklong['data'] = data\nr = klong('1+data')\nassert r == 1 + data\n</code></pre> <p>Variables may be directly retrieved from KlongPy context:</p> <pre><code>klong('Q::1+data')\nQ = klong['Q']\nprint(Q)\n</code></pre>"},{"location":"python_integration/#python-library-access","title":"Python library access","text":"<p>Python functions, including lambdas, can be easily added to support common operations.</p> <p>In order to be consistent with the Klong language, the parameters of Python functions may have at most three parameters and they must be x, y, and z.</p> <pre><code>from datetime import datetime\nfrom klongpy import KlongInterpreter\nklong = KlongInterpreter()\nklong['strptime'] = lambda x: datetime.strptime(x, \"%d %B, %Y\")\nklong(\"\"\"\n    a::strptime(\"21 June, 2018\")\n    .p(a)\n    d:::{};d,\"timestamp\",a\n    .p(d)\n\"\"\")\n</code></pre> <p>prints the following dictionary to the console:</p> <pre><code>2018-06-21 00:00:00\n{'timestamp': datetime.datetime(2018, 6, 21, 0, 0)}\n</code></pre> <p>You can go one step further and call back into Python from Klong with the result:</p> <p><pre><code>from datetime import datetime\nfrom klongpy import KlongInterpreter\nklong = KlongInterpreter()\nklong['strptime'] = lambda x: datetime.strptime(x, \"%d %B, %Y\")\nklong['myprint'] = lambda x: print(f\"called from KlongPy: {x}\")\nklong(\"\"\"\n    a::strptime(\"21 June, 2018\")\n    myprint(a)\n    d:::{};d,\"timestamp\",a\n    myprint(d)\n\"\"\")\n</code></pre> outputs <pre><code>called from KlongPy: 2018-06-21 00:00:00\ncalled from KlongPy: {'timestamp': datetime.datetime(2018, 6, 21, 0, 0)}\n</code></pre></p>"},{"location":"python_integration/#loading-python-modules-directly-into-klongpy","title":"Loading Python Modules directly into KlongPy","text":"<p>KlongPy has the powerful ability to load Python modules directly. This can be extremely useful when you want to utilize the functionality offered by various Python libraries, and seamlessly integrate them into your KlongPy programs.  </p> <p>Here is an example of how you can load a Python module into KlongPy:</p> <pre><code>$ rlwrap kgpy\n\nWelcome to KlongPy REPL v0.7.0\nauthor: Brian Guarraci\nrepo  : https://github.com/briangu/klongpy\nCtrl-D or ]q to quit\n\n?&gt; .py(\"math\")\n1\n?&gt; sqrt(64)\n8.0\n?&gt; fsum(!100)\n4950.0\n</code></pre> <p>In order to keep consistency with Klong 3-parameter function rules, KlongPy will attempt to remap loaded functions to use the x,y and z convention.  For example, in the Python math module, fsum is defined as fsum(seq), so KlongPy remaps this to fsum(x) so that it works within the runtime.</p>"},{"location":"python_integration/#loading-custom-python-modules","title":"Loading Custom Python Modules","text":"<p>Custom modules can be written for KlongPy in the same way as any Python module, the main difference is that they don't need to be installed (e.g. via pip).</p> <p>Simply create a directory with a init.py and appropriate files, as in:</p> <pre><code># __init__.py\nfrom .hello_world import hello\n</code></pre> <pre><code># hello_world.py\n\ndef hello():\n    return \"world!\"\n\ndef not_exported():\n    raise RuntimeError()\n</code></pre> <p>Now, you can import the module with the .py command and run the \"hello\" function.</p> <pre><code>$ rlwrap kgpy\n\nWelcome to KlongPy REPL v0.7.0\nauthor: Brian Guarraci\nrepo  : https://github.com/briangu/klongpy\nCtrl-D or ]q to quit\n\n?&gt; .py(\"tests/plugins/greetings\")\n1\n?&gt; hello()\nworld!\n</code></pre>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Welcome to KlongPy! Get up and running with KlongPy in just a few steps.</p>"},{"location":"quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.9 or higher (3.11 or 3.12 recommended)</li> <li><code>pip</code> for installing packages</li> </ul>"},{"location":"quick-start/#setup-steps","title":"Setup Steps","text":"<p>It's a good idea to work inside a virtual environment so that the dependencies for KlongPy don't interfere with other Python projects:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>With the environment active, choose one of the install options below (and optionally <code>rlwrap</code> for a nicer REPL experience):</p> <pre><code># Base runtime only (NumPy backend)\npip install klongpy\n\n# REPL support (required for `kgpy`)\npip install \"klongpy[repl]\"\n\n# Full feature set (REPL, torch, web, db, websockets)\npip install \"klongpy[all]\"\n</code></pre>"},{"location":"quick-start/#pytorch-backend-optional","title":"PyTorch Backend (Optional)","text":"<p>For GPU acceleration and automatic differentiation, install PyTorch:</p> <pre><code>pip install \"klongpy[torch]\"\n</code></pre> <p>For nicer line editing, install <code>rlwrap</code> via your OS package manager (optional).</p> <p>Then enable the torch backend when running:</p> <pre><code>USE_TORCH=1 kgpy\n# or\nKLONGPY_BACKEND=torch kgpy\n</code></pre>"},{"location":"quick-start/#setting-up-your-first-klongpy-session","title":"Setting Up Your First KlongPy Session","text":"<p>After installing KlongPy, you can start using it right away. Here\u2019s how to set up a basic session:</p> <pre><code>$&gt; rlwrap kgpy\nWelcome to KlongPy REPL v0.7.0\nauthor: Brian Guarraci\nweb   : http://klongpy.org\n]h for help\nCtrl-D or ]q to quit\n\n?&gt; 1+1\n2\n?&gt;\n</code></pre> <p>Version numbers may differ depending on the release you installed.</p> <p>See the REPL Reference for more information on commands and operations.</p>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've installed KlongPy and run a simple session, you're ready to dive deeper. Check out the following resources:</p> <ul> <li>Examples - hands-on code examples.</li> <li>Python Integration - interop with Python modules and data.</li> <li>PyTorch Backend &amp; Autograd - gradients, Jacobians, and compilation.</li> <li>Operators - language operator reference.</li> <li>Performance - benchmarking and backend tips.</li> </ul> <p>For any issues or questions, visit our Issues page on GitHub.</p> <p>Thank you for choosing KlongPy, and happy coding!</p>"},{"location":"repl/","title":"REPL Reference","text":"<p>KlongPy comes with an interactive Read Eval Print Loop (REPL) which is helpful for experimenting with the language.</p> <p>Install the REPL extras first:</p> <pre><code>pip install \"klongpy[repl]\"\n# or\npip install \"klongpy[all]\"\n</code></pre> <p>Launch the REPL using <code>kgpy</code> (installing <code>rlwrap</code> via your OS package manager is recommended for command history and line editing):</p> <pre><code>$ rlwrap kgpy\n</code></pre> <p>Once running you can evaluate expressions directly:</p> <pre><code>?&gt; 1+1\n2\n</code></pre> <p>Several system commands prefixed with <code>]</code> are available:</p> Command Description <code>]h topic</code> Show help text for an operator or topic. <code>]! cmd</code> Run a shell command. <code>]a topic</code> Search help topics (currently a no-op). <code>]i dir</code> List <code>.kg</code> files in a directory. Defaults to <code>KLONGPATH</code>. <code>]l file</code> Load and execute a Klong source file. <code>]T expr</code> Time the evaluation of an expression. Use <code>]T:n</code> to repeat <code>n</code> times. <code>]q</code> Exit the REPL. <p>Press <code>Ctrl-D</code> to quit as well.</p>"},{"location":"repl/#cli-options","title":"CLI options","text":"<p>The <code>kgpy</code> executable supports a few useful flags:</p> <ul> <li><code>-e EXPR</code> Evaluate an expression and exit.</li> <li><code>-l FILE</code> Load a file into the REPL on startup.</li> <li><code>-s PORT</code> Start the IPC server (e.g., <code>kgpy -s 8888</code>).</li> <li><code>-t FILE</code> Run a test file (non-interactive).</li> <li><code>-v</code> Verbose output.</li> </ul>"},{"location":"table_and_key_value_stores/","title":"Table and Key-Value Stores","text":"<p>To support the KlongPy database capabilities, the <code>klongpy.db</code> module includes a key-value store capability that allows for saving and retrieving tables from disk. There is a more generic key-value store as well as a TableStore. The TableStore merges tables when writing to disk, while the generic key-value store writes raw serialized data and doesn't consider the contents.</p> <p>Install the database extras first:</p> <pre><code>pip install \"klongpy[db]\"\n</code></pre> <p>Key-value stores operate as dictionaries, so setting a value updates the contents on disk and reading a value retrieves it.  Similar to Klong dictionaries, if the value does not exist, then the undefined value is returned.</p>"},{"location":"table_and_key_value_stores/#tablestore","title":"TableStore","text":"<p>Since KlongPy Tables are backed by Pandas DataFrames, it's convenient to be able to save/load them from disk.  For this we use the .tables() command.  If table is already present on disk, then the set results in the merge of the two DataFrames.</p> <p>Let's consider that we have a table called 'prices' and we want to store it on disk.</p> <pre><code>?&gt; tbs::.tables(\"/tmp/tables\")\n/tmp/tables:tables\n?&gt; tbs,\"prices\",prices\n/tmp/tables:tables\n</code></pre> <p>Similarly, reading values is the same as getting a value from a dict:</p> <pre><code>?&gt; prices::tbs?\"prices\"\n</code></pre>"},{"location":"table_and_key_value_stores/#generic-key-value-store","title":"Generic key-value store","text":"<p>A simple key-value store backed by disk is available via the .kvs() command.</p> <pre><code>?&gt; kvs::.kvs(\"/tmp/kvs\")\n/tmp/kvs:kvs\n?&gt; kvs,\"hello\",,\"world\"\n/tmp/kvs:kvs\n</code></pre> <p>Now a file <code>/tmp/kvs/hello</code> exists with a pickled instance of \"hello\". Only unpickle data you trust.</p> <p>Retrieving a value is the same as reading from a dictionary:</p> <pre><code>?&gt; kvs?\"hello\"\nworld\n</code></pre>"},{"location":"timer/","title":"Timer","text":"<p>KlongPy includes periodic timer capabilities:</p> <pre><code>cb::{.p(\"hello\")}\nth::.timer(\"greeting\";1;cb)\n</code></pre> <p>To stop the timer, it can be closed via:</p> <pre><code>.timerc(th)\n</code></pre> <p>The following example will create a timer which counts to 5 and then  terminates the timer by return 0 from the callback.</p> <pre><code>counter::0\nu::{counter::counter+1;.p(counter);1}\nc::{.p(\"stopping timer\");0}\ncb::{:[counter&lt;5;u();c()]}\nth::.timer(\"count\";1;cb)\n</code></pre> <p>which displays:</p> <pre><code>1\n2\n3\n4\n5\nstopping timer\n</code></pre>"},{"location":"torch_backend/","title":"PyTorch Backend and Autograd","text":"<p>KlongPy supports multiple array backends. The PyTorch backend enables GPU acceleration and automatic differentiation (autograd) for gradient-based computations.</p>"},{"location":"torch_backend/#enabling-the-pytorch-backend","title":"Enabling the PyTorch Backend","text":"<p>Set the <code>USE_TORCH</code> environment variable:</p> <pre><code># Enable torch backend\nUSE_TORCH=1 python your_script.py\n\n# Or in the REPL\nUSE_TORCH=1 kgpy\n</code></pre> <p>You can also set:</p> <pre><code>KLONGPY_BACKEND=torch\n</code></pre> <p>Or programmatically:</p> <pre><code>import os\nos.environ['USE_TORCH'] = '1'\n\nfrom klongpy import KlongInterpreter\nklong = KlongInterpreter()\nprint(klong._backend.name)  # 'torch'\n</code></pre> <p>You can also pass the backend directly:</p> <pre><code>from klongpy import KlongInterpreter\nklong = KlongInterpreter(backend=\"torch\", device=\"cuda\")\n</code></pre>"},{"location":"torch_backend/#backend-comparison","title":"Backend Comparison","text":"Feature NumPy Backend PyTorch Backend Default Yes No (requires USE_TORCH=1 or KLONGPY_BACKEND=torch) Object dtype Yes No String operations Yes Not supported GPU acceleration No Yes (CUDA/MPS) Autograd Numeric only Native autograd Small array performance Faster Slightly slower Large array performance Good Better (especially on GPU)"},{"location":"torch_backend/#performance","title":"Performance","text":"<p>The torch backend excels with large arrays:</p> <pre><code>Benchmark              NumPy      Torch      Winner\n---------------------------------------------------------\nvector_add_100K        0.04ms     0.08ms     NumPy (2x)\nvector_add_1M          0.36ms     0.07ms     Torch (5x)\ncompound_expr_1M       0.61ms     0.07ms     Torch (8x)\ngrade_up_100K          0.59ms     0.19ms     Torch (3x)\n</code></pre> <p>For small arrays (&lt;100K elements), NumPy is slightly faster due to lower dispatch overhead. For larger arrays, torch wins significantly.</p>"},{"location":"torch_backend/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>KlongPy provides several gradient and differentiation operators:</p>"},{"location":"torch_backend/#typing-special-characters","title":"Typing Special Characters","text":"Symbol Name Mac Windows <code>\u2207</code> Nabla Character Viewer (Ctrl+Cmd+Space) Alt+8711 <code>\u2202</code> Partial Option + d Alt+8706 <p>On Mac, <code>\u2202</code> can be typed directly with Option + d. For <code>\u2207</code>, use the Character Viewer or copy-paste.</p>"},{"location":"torch_backend/#autograd-operator-recommended","title":"<code>:&gt;</code> Autograd Operator (Recommended)","text":"<p>The <code>:&gt;</code> operator uses PyTorch autograd for exact gradients:</p> <pre><code>f::{x^2}         :\" Define f(x) = x^2\nf:&gt;3             :\" Compute f'(3) = 6.0\n</code></pre> <p>The syntax is <code>function:&gt;point</code> where: - <code>function</code> is a scalar-valued function (must return a single number) - <code>point</code> is the input at which to compute the gradient</p>"},{"location":"torch_backend/#numeric-gradient-operator","title":"<code>\u2207</code> Numeric Gradient Operator","text":"<p>The <code>\u2207</code> operator always uses numeric differentiation (finite differences), regardless of backend:</p> <pre><code>f::{x^2}         :\" Define f(x) = x^2\n3\u2207f              :\" Compute f'(3) \u2248 6.0\n</code></pre> <p>The syntax is <code>point\u2207function</code> (note: reversed order from <code>:&gt;</code>).</p>"},{"location":"torch_backend/#how-they-work","title":"How They Work","text":"Operator Method Precision Speed <code>:&gt;</code> with torch PyTorch autograd Exact Fast <code>:&gt;</code> without torch Numeric ~1e-6 error Slower <code>\u2207</code> (any backend) Always numeric ~1e-6 error Slower <p>With the torch backend (<code>USE_TORCH=1</code> or <code>KLONGPY_BACKEND=torch</code>), prefer <code>:&gt;</code> for: - Exact gradients (no floating-point approximation error) - Complex computational graphs - Better performance on large arrays</p>"},{"location":"torch_backend/#examples","title":"Examples","text":"<p>Scalar function: <pre><code>f::{x^3}          :\" f(x) = x^3\nf:&gt;2              :\" f'(2) = 3*4 = 12.0\n</code></pre></p> <p>Polynomial: <pre><code>p::{((3*x^4)-(2*x^2))+x}   :\" p(x) = 3x^4 - 2x^2 + x\np:&gt;1                        :\" p'(1) = 12 - 4 + 1 = 9.0\n</code></pre></p> <p>Vector function (sum of squares): <pre><code>g::{+/x^2}             :\" g(x) = sum(x_i^2)\ng:&gt;[1.0 2.0 3.0]       :\" [2 4 6] = 2*x\n</code></pre></p> <p>Gradient descent: <pre><code>f::{x^2}\nx::5.0\nlr::0.1\n\n:\" Update rule: x = x - lr * grad\nx::x-(lr*f:&gt;x)\n</code></pre></p>"},{"location":"torch_backend/#multi-parameter-gradients","title":"Multi-Parameter Gradients","text":"<p>Compute gradients for multiple parameters simultaneously using a list of symbols:</p> <pre><code>w::2.0\nb::3.0\nloss::{(w^2)+(b^2)}\n\n:\" Compute gradients for both w and b\ngrads::loss:&gt;[w b]    :\" [4.0 6.0] = [2w, 2b]\n</code></pre> <p>This is especially useful for neural network training:</p> <pre><code>w::1.0\nb::0.0\nX::[1 2 3]\nY::[3 5 7]\n\n:\" MSE loss\nloss::{(+/((w*X)+b-Y)^2)%3}\n\n:\" Compute both gradients in one call\ngrads::loss:&gt;[w b]\n</code></pre>"},{"location":"torch_backend/#jacobian-computation","title":"Jacobian Computation","text":"<p>Compute the Jacobian matrix (matrix of partial derivatives) using the <code>\u2202</code> operator or <code>.jacobian()</code> function:</p> <pre><code>f::{x^2}                 :\" Element-wise square\n\n:\" Using \u2202 operator (point\u2202function)\n[1 2]\u2202f                  :\" [[2 0] [0 4]] diagonal matrix\n\n:\" Using .jacobian() function\n.jacobian(f;[1 2])       :\" Same result\n</code></pre> <p>For vector-valued functions f: R^n -&gt; R^m, the Jacobian is an m x n matrix where J[i,j] = df_i/dx_j.</p>"},{"location":"torch_backend/#multi-parameter-jacobians","title":"Multi-Parameter Jacobians","text":"<p>Just like gradients, you can compute Jacobians with respect to multiple parameters using a list of symbols:</p> <pre><code>w::[1.0 2.0]\nb::[3.0 4.0]\nf::{w^2}                 :\" Returns [w0^2, w1^2]\n\n:\" Compute Jacobians for both w and b\njacobians::[w b]\u2202f       :\" Returns [J_w, J_b]\n</code></pre> <p>This returns a list of Jacobian matrices, one per parameter. Useful for analyzing how vector-valued functions depend on multiple parameter sets.</p>"},{"location":"torch_backend/#custom-optimizers","title":"Custom Optimizers","text":"<p>KlongPy provides the gradient primitives (<code>:&gt;</code>, <code>\u2202</code>, <code>.jacobian()</code>). For optimizers, use the example classes in <code>examples/autograd/optimizers.py</code> which you can copy to your project and customize.</p> <p>Manual gradient descent (no optimizer needed): <pre><code>w::10.0\nloss::{w^2}\nlr::0.1\n\n:\" Update rule: w = w - lr * gradient\n{w::w-(lr*loss:&gt;w)}'!50\nw                        :\" Close to 0\n</code></pre></p> <p>Using a custom optimizer class:</p> <ol> <li>Copy <code>examples/autograd/optimizers.py</code> to your project directory</li> <li>Import with <code>.pyf()</code>:</li> </ol> <pre><code>:\" Import the optimizer class\n.pyf(\"optimizers\";\"SGDOptimizer\")\n\n:\" Setup parameters and loss\nw::10.0\nloss::{w^2}\n\n:\" Create optimizer with learning rate 0.1\nopt::SGDOptimizer(klong;[\"w\"];:{[\"lr\" 0.1]})\n\n:\" Run optimization steps\n{opt(loss)}'!50\nw                        :\" Close to 0\n</code></pre> <p>Available example optimizers: - <code>SGDOptimizer</code> - Stochastic Gradient Descent with optional momentum - <code>AdamOptimizer</code> - Adam optimizer with adaptive learning rates</p> <p>SGD with momentum: <pre><code>.pyf(\"optimizers\";\"SGDOptimizer\")\nopt::SGDOptimizer(klong;[\"w\"];:{[\"lr\" 0.01 \"momentum\" 0.9]})\n</code></pre></p> <p>Adam optimizer: <pre><code>.pyf(\"optimizers\";\"AdamOptimizer\")\nopt::AdamOptimizer(klong;[\"w\" \"b\"];:{[\"lr\" 0.001]})\n</code></pre></p> <p>Training loop example: <pre><code>.pyf(\"optimizers\";\"AdamOptimizer\")\n\nw::1.0;b::0.0\nX::[1 2 3];Y::[3 5 7]\nloss::{(+/((w*X)+b-Y)^2)%3}\nopt::AdamOptimizer(klong;[\"w\" \"b\"];:{[\"lr\" 0.1]})\n\n:\" Train for 500 steps\n{opt(loss)}'!500\n</code></pre></p> <p>Creating your own optimizer:</p> <p>The example optimizers use <code>multi_grad_of_fn</code> from <code>klongpy.autograd</code> to compute gradients for multiple parameters. Copy and modify the optimizer classes to implement custom update rules (RMSprop, AdaGrad, learning rate schedules, etc.).</p>"},{"location":"torch_backend/#gpu-acceleration","title":"GPU Acceleration","text":"<p>When CUDA or Apple MPS is available, tensors automatically use GPU:</p> <pre><code>from klongpy import KlongInterpreter\nimport os\nos.environ['USE_TORCH'] = '1'\n\nklong = KlongInterpreter()\nprint(klong._backend.device)  # 'cuda:0', 'mps:0', or 'cpu'\n</code></pre>"},{"location":"torch_backend/#device-selection","title":"Device Selection","text":"<p>The backend automatically selects the best available device: 1. CUDA (NVIDIA GPU) - if available 2. MPS (Apple Silicon) - if available 3. CPU - fallback</p>"},{"location":"torch_backend/#mps-limitations","title":"MPS Limitations","text":"<p>Apple's MPS backend has some limitations: - No float64 support (uses float32) - Some operations fall back to CPU</p>"},{"location":"torch_backend/#mixing-with-python","title":"Mixing with Python","text":"<p>Access torch tensors directly:</p> <pre><code>from klongpy import KlongInterpreter\nimport os\nos.environ['USE_TORCH'] = '1'\n\nklong = KlongInterpreter()\n\n# KlongPy operations return torch tensors\nresult = klong('2*1+!1000000')\nprint(type(result))  # &lt;class 'torch.Tensor'&gt;\nprint(result.device)  # cuda:0, mps:0, or cpu\n\n# Convert to numpy when needed\nimport numpy as np\nnp_result = result.cpu().numpy()\n</code></pre>"},{"location":"torch_backend/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use torch for large computations: Switch to torch backend for arrays &gt;100K elements</p> </li> <li> <p>Keep data as tensors: Avoid unnecessary conversions between numpy and torch</p> </li> <li> <p>Batch operations: Combine operations to minimize dispatch overhead</p> </li> <li> <p>Use autograd for gradients: Native autograd is faster and more accurate than numeric differentiation</p> </li> </ol>"},{"location":"torch_backend/#function-compilation","title":"Function Compilation","text":"<p>The torch backend supports compiling Klong functions for optimized execution using <code>torch.compile</code>:</p>"},{"location":"torch_backend/#compilefninput-compile-function","title":"<code>.compile(fn;input)</code> - Compile Function","text":"<p>Compiles a function for faster execution:</p> <pre><code>f::{x^2}\ncf::.compile(f;3.0)      :\" Returns compiled function\ncf(5.0)                   :\" 25.0 (optimized)\n</code></pre> <p>The compiled function runs significantly faster for complex computations.</p>"},{"location":"torch_backend/#exportfninputpath-export-computation-graph","title":"<code>.export(fn;input;path)</code> - Export Computation Graph","text":"<p>Exports the function's computation graph to a file for inspection:</p> <pre><code>f::{(x^3)+(2*x^2)+x}\ninfo::.export(f;2.0;\"model.pt2\")\n.p(info@\"graph\")         :\" Print computation graph\n</code></pre> <p>Returns a dictionary with: - <code>\"compiled_fn\"</code> - The compiled function - <code>\"export_path\"</code> - Path where graph was saved - <code>\"graph\"</code> - String representation of computation graph</p> <p>The exported <code>.pt2</code> file can be loaded with <code>torch.export.load()</code> in Python.</p>"},{"location":"torch_backend/#compilexfninputoptions-extended-compilation","title":"<code>.compilex(fn;input;options)</code> - Extended Compilation","text":"<p>Compile with advanced options for mode and backend:</p> <pre><code>f::{x^2}\n\n:\" Fast compilation for development\ncf::.compilex(f;3.0;:{[\"mode\" \"reduce-overhead\"]})\n\n:\" Maximum optimization for production\ncf::.compilex(f;3.0;:{[\"mode\" \"max-autotune\"]})\n\n:\" Debug mode (no compilation)\ncf::.compilex(f;3.0;:{[\"backend\" \"eager\"]})\n</code></pre> <p>Options dictionary: - <code>\"mode\"</code> - Compilation mode (see table below) - <code>\"backend\"</code> - Compilation backend (see table below) - <code>\"fullgraph\"</code> - Set to 1 to require full graph compilation - <code>\"dynamic\"</code> - Set to 1 for dynamic shapes, 0 for static</p>"},{"location":"torch_backend/#cmodes-query-compilation-modes","title":"<code>.cmodes()</code> - Query Compilation Modes","text":"<p>Get information about available modes and backends:</p> <pre><code>info::.cmodes()\n.p(info@\"modes\")          :\" Available compilation modes\n.p(info@\"backends\")       :\" Available backends\n.p(info@\"recommendations\") :\" Suggested settings\n</code></pre>"},{"location":"torch_backend/#compilation-mode-comparison","title":"Compilation Mode Comparison","text":"Mode Compile Time Runtime Speed Best For <code>default</code> Medium Good General use <code>reduce-overhead</code> Fast Moderate Development/testing <code>max-autotune</code> Slow Best Production"},{"location":"torch_backend/#backend-comparison_1","title":"Backend Comparison","text":"Backend Description <code>inductor</code> Default - C++/Triton code generation (fastest) <code>eager</code> No compilation - runs original Python (debugging) <code>aot_eager</code> Ahead-of-time eager (debugging + autograd) <code>cudagraphs</code> CUDA graphs - reduces GPU kernel launch overhead <p>Note: Compilation requires a C++ compiler on your system. Use <code>\"backend\" \"eager\"</code> to bypass compilation for debugging. If compilation fails, an error message will indicate the issue.</p>"},{"location":"torch_backend/#gradient-verification","title":"Gradient Verification","text":"<p>Use <code>.gradcheck()</code> to verify that autograd gradients are correct:</p>"},{"location":"torch_backend/#gradcheckfninputs-verify-gradients","title":"<code>.gradcheck(fn;inputs)</code> - Verify Gradients","text":"<p>Verifies autograd gradients against numeric gradients:</p> <pre><code>f::{x^2}\n.gradcheck(f;3.0)        :\" Returns 1 if correct\n\ng::{+/x^2}\n.gradcheck(g;[1.0 2.0 3.0])  :\" Returns 1\n</code></pre> <p>This uses <code>torch.autograd.gradcheck</code> internally for rigorous verification.</p> <p>Use cases: - Verifying custom gradient implementations - Debugging gradient computation issues - Ensuring numerical stability</p>"},{"location":"torch_backend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"torch_backend/#pytorch-backend-does-not-support-object-dtype","title":"\"PyTorch backend does not support object dtype\"","text":"<p>The torch backend cannot handle mixed-type arrays or nested structures with varying shapes. Use the numpy backend for these cases.</p>"},{"location":"torch_backend/#mps-float64-errors","title":"MPS float64 errors","text":"<p>MPS doesn't support float64. The backend automatically converts to float32, but some precision-sensitive operations may behave differently.</p>"},{"location":"torch_backend/#slow-small-array-operations","title":"Slow small array operations","text":"<p>For arrays &lt;10K elements, numpy may be faster. Consider using numpy backend for small array workloads or batching small operations together.</p>"},{"location":"torch_backend/#torchcompile-errors","title":"torch.compile errors","text":"<p>If <code>.compile()</code> fails with C++ errors, ensure you have: - A C++ compiler installed (clang++ or g++) - The required header files (may need Xcode Command Line Tools on macOS)</p>"},{"location":"web_server/","title":"Web Server","text":"<p>KlongPy includes a simple web server module. It's optional so you need to install the dependencies:</p> <pre><code>pip3 install \"klongpy[web]\"\n</code></pre> <p>The web server allows you to implement KlongPy functions as GET/POST handlers for registered routes. Handlers must be monadic functions (arity 1).</p> <p>Here's a simple example that lets you fetch and update a data array:</p> <pre><code>:\" Import the Klongpy web module.  Requires pip3 install klongpy[web] first\"\n.py(\"klongpy.web\")\n\n:\" Array of data to display\"\ndata::[]\n\n:\" Return the data for a GET method at /\"\nindex::{x;data}\n\n:\" Create the GET route handlers\"\nget:::{}\nget,\"/\",index\n\n:\" Append the query param q value to data\"\nupdate::{[p];p::x?\"p\";.p(p);data::data,p}\n\n:\" Create the POST route handlers\"\npost:::{}\npost,\"/p\",update\n\n:\" Start the web server with the GET and POST handlers\"\n.web(8888;get;post)\n\n.p(\"curl -X POST -d\\\"p=100\\\" \\\"http://localhost:8888/p\\\"\")\n.p(\"curl \\\"http://localhost:8888\\\"\")\n\ndata\n</code></pre> <p>Test it out:</p> <pre><code>$ curl \"http://localhost:8888\"\n[]\n$ curl -X POST -d\"p=100\" \"http://localhost:8888/p\"\n[100]\n$ curl \"http://localhost:8888\"\n[100]\n</code></pre> <p>You can also launch the same web server directly from the REPL:</p> <pre><code>?&gt; .py(\"klongpy.web\")\n?&gt; data::!10\n?&gt; index::{x; \"Hello, Klong World! \", data}\n?&gt; get:::{}; get,\"/\",index\n?&gt; post:::{}\n?&gt; h::.web(8888;get;post)\n</code></pre> <p>Now in another terminal:</p> <pre><code>$ curl http://localhost:8888\n['Hello, Klong World! ' 0 1 2 3 4 5 6 7 8 9]\n</code></pre> <p>Stop the server with:</p> <pre><code>?&gt; .webc(h)\n1\n</code></pre>"},{"location":"websockets/","title":"WebSockets","text":"<p>KlongPy includes an optional WebSocket client module that lets you connect to WebSocket servers and handle messages from KlongPy code.</p> <p>Install the WebSocket extras first:</p> <pre><code>pip install \"klongpy[ws]\"\n</code></pre>"},{"location":"websockets/#client-basics","title":"Client Basics","text":"<p>Load the module and define a message handler before connecting:</p> <pre><code>.py(\"klongpy.ws\")\n\n:\" Message handler (dyad: x=client, y=message)\"\n.ws.m::{[c msg]; .d(\"recv: \"); .p(msg)}\n\n:\" Connect to a WebSocket URI\"\nws::.ws(\"ws://localhost:8765\")\n\n:\" Send a message (encoded as JSON)\"\nws([\"ping\" 1])\n\n:\" Close the connection\"\n.wsc(ws)\n</code></pre>"},{"location":"websockets/#message-encoding","title":"Message Encoding","text":"<p>Messages are encoded/decoded as JSON. NumPy arrays are converted to lists on send, and JSON arrays are decoded back into Klong lists.</p>"},{"location":"websockets/#required-handler","title":"Required Handler","text":"<p>Incoming messages always invoke <code>.ws.m</code>. If you don\u2019t define it, the client will raise an error on receipt. The handler should be a dyadic function:</p> <pre><code>.ws.m::{[c msg]; .p(\"message: \"); .p(msg)}\n</code></pre>"},{"location":"websockets/#notes","title":"Notes","text":"<ul> <li>The current implementation provides a client only (server support is not yet exposed).</li> <li><code>.ws</code> expects a full WebSocket URI (for example, <code>ws://localhost:8765</code>).</li> </ul>"}]}